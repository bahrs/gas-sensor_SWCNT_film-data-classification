{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def make_LSTM(X, y, n_LSTM_layers: int = 1, n_units: int = 64, dropout: float = 0, optimizer: tf.keras.optimizers = Adam, learning_rate: float = 0.001, loss: str = 'mean_squared_error'):\n",
    "    input_shape = ((X[0]).shape[1], X[0].shape[2])\n",
    "    output_shape = y[0].shape[1]\n",
    "    to_return_sequences = lambda n_LSTM_layers_left: True if n_LSTM_layers_left > 1 else False\n",
    "\n",
    "    model = Sequential()\n",
    "    for n_LSTM_layers_left in range(n_LSTM_layers, 0, -1):\n",
    "        model.add(LSTM(units=n_units, \n",
    "                       return_sequences=to_return_sequences(n_LSTM_layers_left), \n",
    "                       input_shape=input_shape, \n",
    "                       dropout = dropout, \n",
    "                       recurrent_dropout=dropout))\n",
    "        n_units = n_units // 2 if n_units > 8 else n_units\n",
    "    model.add(Dense(units=output_shape))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer(learning_rate=learning_rate), weighted_metrics = [])\n",
    "\n",
    "    return model\n",
    "\n",
    "def fit_LSTM(model, X_train, y_train, X_test, y_test, sw_train, sw_test, epochs: int = 100, batch_size: int = 32, return_history: bool = False):\n",
    "    patience = 30\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=patience, min_delta=0.1, restore_best_weights=True)]\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data = (X_test, y_test),\n",
    "        #validation_data=(X_test, y_test, sw_test),  # failed experiments with sample weighting\n",
    "        #sample_weight = sw_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=0,\n",
    "        callbacks = callbacks,\n",
    "        shuffle = False,\n",
    "        )\n",
    "    #test_loss = model.evaluate(X_test, y_test, sample_weight = sw_test, batch_size = batch_size)\n",
    "    y_pred = model.predict(X_test, batch_size = batch_size, verbose = 0,\n",
    "                           #workers = 16, use_multiprocessing = True,\n",
    "                           )\n",
    "    test_rmse = mean_squared_error(y_test, y_pred, squared = False, multioutput = 'uniform_average')\n",
    "    #test_rmse = mean_squared_error(y_test, y_pred, sample_weight= sw_test, squared = False, multioutput = 'uniform_average')\n",
    "    ### taking history into account and trying to minimize the gap between train and validation loss\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    mse_epochs = [(x-y)**2 for x, y in zip(train_loss, val_loss)]\n",
    "    history_slice = slice(-patience,len(mse_epochs),1)# slice(-3*patience//2,-patience//2,1)  # apparently it doesn't bug out if len(mse_epochs) < 2* patience\n",
    "    historical_loss_rmse = np.sqrt(np.nanmean(mse_epochs[history_slice]))  # this slice was chosen as it contains the most optimized hyperparameters\n",
    "    # here a new metric - historical_loss_rmse was introduced, that shows how far validation and training loss at the last patience epochs are\n",
    "\n",
    "    hist_test_weights = [10,0]  # completely arbitrary parameter that sets up a weighted sum of traditional RMSE and HLRMSE with weights 2 and 1 respectively\n",
    "    test_loss = (np.array([test_rmse, historical_loss_rmse]) * np.array(hist_test_weights)).sum() / sum(hist_test_weights)\n",
    "    if return_history:\n",
    "        return test_loss, history.history\n",
    "    else:\n",
    "        return test_loss, None\n",
    "\n",
    "def objective_LSTM(trial):\n",
    "    # Load_data\n",
    "    data_load_params = dict(\n",
    "        dedrifting_method = trial.suggest_categorical('dedrifting_method', ['SavGol', 'exp', \"none\"]), # 'SavGol', \n",
    "        )\n",
    "    if data_load_params['dedrifting_method'] == 'SavGol':\n",
    "        data_load_params['window_length'] = trial.suggest_int('window_length', 150, 500, step = 10)\n",
    "        data_load_params['envelope_choice'] = trial.suggest_categorical('envelope_choice', ['multienv', 'topenv'])\n",
    "        data_load_params['alpha'] = 1\n",
    "    elif data_load_params['dedrifting_method'] == 'exp':\n",
    "        data_load_params['alpha'] = trial.suggest_float('alpha', 0.001, 0.1, log = True)\n",
    "        data_load_params['window_length'] = 1\n",
    "        data_load_params['envelope_choice'] = trial.suggest_categorical('envelope_choice', ['multienv', 'topenv'])\n",
    "    elif data_load_params['dedrifting_method'] == 'none':\n",
    "        data_load_params['envelope_choice'] = 'none'\n",
    "        data_load_params['window_length'] = 1\n",
    "        data_load_params['alpha'] = 1\n",
    "    \n",
    "    params = dict(\n",
    "        look_back = trial.suggest_int('look_back', 20, 57, log=False),\n",
    "        n_components = trial.suggest_int('n_components', 15, 150),\n",
    "        do_PCA = True,  # trial.suggest_categorical('do_PCA', [True, False]),\n",
    "        n_LSTM_layers = 1,  # trial.suggest_int('n_LSTM_layers', 1, 2),\n",
    "        n_units = trial.suggest_int('n_units', 16, 128, log=False),  # trial.suggest_categorical('n_units', [16, 32, 64, 96, 128]),  \n",
    "        dropout = trial.suggest_float('dropout', 0.005, 0.5, log=False),  # 0.162\n",
    "        learning_rate= trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        epochs = 150,  # trial.suggest_int('epochs', 50, 300),\n",
    "        batch_size = trial.suggest_categorical('batch_size', [64, 128, 256]),\n",
    "        )\n",
    "    \n",
    "\n",
    "    DF = load_full_dedrifted_dataset(**data_load_params)  # custom function for loading and preprocessing the data\n",
    "    # Split data and perform PCA\n",
    "    train_X, test_X, train_y, test_y, train_SW, test_SW = train_test_RNN(  # my custom version of train_test_split for sequential data\n",
    "        DF, \n",
    "        look_back= params['look_back'],\n",
    "        n_components= params['n_components'],\n",
    "        do_PCA= params['do_PCA'],\n",
    "        start=8)\n",
    "    model = make_LSTM(train_X, \n",
    "                      train_y, \n",
    "                      optimizer = Adam, \n",
    "                      loss = 'mean_squared_error',\n",
    "                      n_LSTM_layers = params['n_LSTM_layers'],\n",
    "                      n_units = params['n_units'],\n",
    "                      dropout = params['dropout'],\n",
    "                      learning_rate = params['learning_rate'],)\n",
    "  \n",
    "    rmse_, history_ = fit_LSTM(\n",
    "        model, train_X[0], train_y[0], test_X[0], test_y[0], \n",
    "        train_SW, test_SW,\n",
    "        epochs = params['epochs'],\n",
    "        batch_size= params['batch_size'], \n",
    "        return_history=True)\n",
    "    plot_history(history_, params)  # custom function to plot the NN training results\n",
    "    \n",
    "    return rmse_\n",
    "\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "study = optuna.create_study(direction= 'minimize',  # minimize  for regression minimize  for classification 'maximize'\n",
    "                            pruner= optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=0, interval_steps=1, n_min_trials=1),\n",
    "                            #optuna.pruners.SuccessiveHalvingPruner(min_resource='auto', reduction_factor=3, min_early_stopping_rate=4, bootstrap_count=0) \n",
    "                            # optuna.pruners.MedianPruner(n_warmup_steps=10),\n",
    "                            )\n",
    "study.optimize(objective_LSTM, n_trials=10000, timeout=8*60*60, show_progress_bar = False, n_jobs=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
