{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New thermocycling ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copying old stuff and examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SavGol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SavGol(trace: np.ndarray, window_length: int = 600, polyorder: int = 3) -> np.ndarray:\n",
    "    baseline_trend = savgol_filter(x = trace, window_length = window_length, polyorder = polyorder, mode = 'mirror')\n",
    "    return trace - baseline_trend\n",
    "\n",
    "def LOWESS(trace: pd.Series, cycles: int = 200) -> np.ndarray:\n",
    "    if cycles >= len(trace): print(\"data do not have enough cycles for this 'cycles' parameter\")\n",
    "    else:\n",
    "        baseline_trend = lowess(trace.values, trace.index, frac=cycles/len(trace), return_sorted=False)\n",
    "        return trace.values - baseline_trend\n",
    "def SavGoldf(df_: pd.DataFrame,\n",
    "             voltage_point: int = 201,\n",
    "             window_length: int = 600,\n",
    "             polyorder: int = 4) -> pd.DataFrame:\n",
    "    '''voltage point: number of the column, corresponding to certain voltage\n",
    "    window_length: smoothing parameter. Typically 200-800\n",
    "    polyorder: 3-4'''\n",
    "    data = df_.iloc[:, :402].values\n",
    "    envelope = df_.iloc[:, voltage_point].values\n",
    "    baseline = savgol_filter(x=envelope,\n",
    "                             window_length=window_length,\n",
    "                             polyorder=polyorder,\n",
    "                             mode='mirror')\n",
    "    baseline_df = [[x] * 402 for x in envelope]\n",
    "    corrected_df = pd.DataFrame(data=data - baseline_df,\n",
    "                                index=df_.index.values,\n",
    "                                columns=list(df_.columns)[:402])\n",
    "    corrected_df = pd.concat([corrected_df, df_.iloc[:,-5:]], axis=1)\n",
    "    return corrected_df\n",
    "def LOWESSdf(df_: pd.DataFrame,\n",
    "             voltage_point: int = 201,\n",
    "             window_length: int = 600,\n",
    "             polyorder: int = 4) -> pd.DataFrame:\n",
    "    '''voltage point: number of the column, corresponding to certain voltage\n",
    "    window_length: smoothing parameter. Typically 200-800\n",
    "    polyorder: 4'''\n",
    "    data = df_.iloc[:, :402].values\n",
    "    envelope = df_.iloc[:, voltage_point].values\n",
    "    index = df_.index.values\n",
    "    baseline = lowess(endog=envelope,\n",
    "                      exog=index,\n",
    "                      frac=window_length / df_.shape[0],\n",
    "                      return_sorted=False)\n",
    "    baseline_df = [[x] * 402 for x in envelope]\n",
    "    corrected_df = pd.DataFrame(data=data - baseline_df,\n",
    "                                index=df_.index.values,\n",
    "                                columns=list(df_.columns)[:402])\n",
    "    corrected_df = pd.concat([corrected_df, df_.iloc[:,-5:]], axis=1)\n",
    "    return corrected_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBoostPruningCallback(object):\n",
    "\n",
    "    def __init__(\n",
    "        self, trial: optuna.trial.Trial, metric: str, valid_name: str = \"validation\"\n",
    "    ) -> None:\n",
    "        self._trial = trial\n",
    "        self._metric = metric\n",
    "        self._valid_name = valid_name\n",
    "        self._pruned = False\n",
    "        self._message = \"\"\n",
    "\n",
    "    def after_iteration(self, info) -> bool:\n",
    "        step = info.iteration - 1\n",
    "        if self._valid_name not in info.metrics:\n",
    "            raise ValueError(\n",
    "                'The entry associated with the validation name \"{}\" '\n",
    "                \"is not found in the evaluation result list {}.\".format(self._valid_name, info)\n",
    "            )\n",
    "        metrics = info.metrics[self._valid_name]\n",
    "        if self._metric not in metrics:\n",
    "            raise ValueError(\n",
    "                'The entry associated with the metric name \"{}\" '\n",
    "                \"is not found in the evaluation result list {}.\".format(self._metric, info)\n",
    "            )\n",
    "        current_score = metrics[self._metric][-1]\n",
    "        self._trial.report(current_score, step=step)\n",
    "        if self._trial.should_prune():\n",
    "            self._message = \"Trial was pruned at iteration {}.\".format(step)\n",
    "            self._pruned = True\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def check_pruned(self) -> None:\n",
    "        \"\"\"Check whether pruend.\"\"\"\n",
    "        if self._pruned:\n",
    "            raise optuna.TrialPruned(self._message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(dfc_pca, pd.Series(labels), test_size=.33, stratify=labels)\n",
    "\n",
    "    param = {\n",
    "        \"objective\": \"MAE\",  # trial.suggest_categorical(\"objective\", [\"MAE\"])\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 100, 3000, step = 100),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.05, 0.4, log = True),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.5, 5, log = True),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.1, 1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"boosting_type\": \"Plain\", # trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),  # \n",
    "        \"bootstrap_type\": \"MVS\",  # trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),  # \n",
    "        #\"subsample\": trial.suggest_float(\"subsample\", 0.1, 1, log=True),  # only valid for Bernoulli\n",
    "        \"used_ram_limit\": \"8gb\",\n",
    "        \"eval_metric\": \"MAE\",\n",
    "    }\n",
    "\n",
    "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1, log=True)\n",
    "\n",
    "    gbm = CatBoostRegressor(**param)\n",
    "\n",
    "    pruning_callback = CatBoostPruningCallback(trial, \"MAE\", \"validation\")\n",
    "    gbm.fit(train_x, train_y, \n",
    "            eval_set=[(valid_x, valid_y)],\n",
    "            verbose=0,\n",
    "            early_stopping_rounds=500,\n",
    "            callbacks=[pruning_callback],\n",
    "           )\n",
    "    pruning_callback.check_pruned()\n",
    "    \n",
    "    return gbm.get_best_score()['validation']['MAE']\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"minimize\"  # pruners.SuccessiveHalvingPruner()\n",
    "    )\n",
    "    study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(f\"  Value: {trial.value:.4f}\")\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        if type(value) == float:\n",
    "            print(f\"    {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multioutput regression\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(dfc_pca, multiregression_labels, test_size=.33, stratify=multiregression_labels)\n",
    "\n",
    "    param = {\n",
    "        \"objective\": \"MultiRMSE\",  # trial.suggest_categorical(\"objective\", [\"MAE\"])\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 100, 2000, step = 100),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.5, log = True),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.5, 7, log = True),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.1, 1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"boosting_type\": \"Plain\", # trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),  # \n",
    "        \"bootstrap_type\": \"MVS\",  # trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),  # \n",
    "        #\"subsample\": trial.suggest_float(\"subsample\", 0.1, 1, log=True),  # only valid for Bernoulli\n",
    "        \"used_ram_limit\": \"8gb\",\n",
    "        \"eval_metric\": \"MultiRMSE\",\n",
    "    }\n",
    "\n",
    "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1, log=True)\n",
    "\n",
    "    gbm = CatBoostRegressor(**param)\n",
    "\n",
    "    pruning_callback = CatBoostPruningCallback(trial, \"MultiRMSE\", \"validation\")\n",
    "    gbm.fit(train_x, train_y, \n",
    "            eval_set=[(valid_x, valid_y)],\n",
    "            verbose=0,\n",
    "            early_stopping_rounds=100,\n",
    "            callbacks=[pruning_callback],\n",
    "           )\n",
    "    pruning_callback.check_pruned()\n",
    "    \n",
    "    return gbm.get_best_score()['validation']['MultiRMSE']\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"minimize\"  # pruners.SuccessiveHalvingPruner()\n",
    "    )\n",
    "    study.optimize(objective, n_trials=1000, show_progress_bar = True)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(f\"  Value: {trial.value:.4f}\")\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        if type(value) == float:\n",
    "            print(f\"    {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study).update_layout(height = 300).show()\n",
    "optuna.visualization.plot_param_importances(study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\").update_layout(height = 300).show()\n",
    "optuna.visualization.plot_slice(study).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем сам keras\n",
    "import keras\n",
    "# Последовательный тип модели\n",
    "from keras.models import Sequential\n",
    "# Импортируем полносвязный слой, слои активации и слой, превращающий картинку в вектор\n",
    "from keras.layers import Dense, Activation, Flatten, LSTM, Embedding\n",
    "from keras.utils import plot_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time splitting data\n",
    "dataset_time_split = 8\n",
    "X_train_ = np.array(df.loc[df['meas_cycle'] < dataset_time_split].iloc[:,:402])\n",
    "y_train = np.array(df.loc[df['meas_cycle'] < dataset_time_split].loc[:,gas])\n",
    "X_test_ = np.array(df.loc[df['meas_cycle'] >= dataset_time_split].iloc[:,:402])\n",
    "y_test = np.array(df.loc[df['meas_cycle'] >= dataset_time_split].loc[:,gas])\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train = scaler.fit_transform(X_train_)\n",
    "X_test = scaler.transform(X_test_)\n",
    "print(f\"X train {X_train.shape}\\nX test {X_test.shape}\\ny train {y_train.shape}\\ny test {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling and encoding\n",
    "encoder = LabelEncoder()\n",
    "labels = keras.utils.to_categorical(encoder.fit_transform(labels), 4)\n",
    "X_train_, X_test, y_train_, y_test = train_test_split(dfc, labels, test_size=.15, stratify=labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_, y_train_, test_size=.25, stratify=y_train_)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_val = scaler.fit_transform(np.array(X_val))\n",
    "X_train = scaler.fit_transform(np.array(X_train))\n",
    "X_test = scaler.transform(np.array(X_test))\n",
    "\n",
    "print(f\"X train {X_train.shape}\\nX val {X_val.shape}\\nX test {X_test.shape}\\ny train {y_train.shape}\\ny val {y_val.shape}\\ny test {y_test.shape}\")\n",
    "#Simple model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Добавляем скрытый полносвязный слой из 64 нейронов\n",
    "model.add(Dense(units=64, activation='relu', input_shape=(X_train.shape[-1],)))\n",
    "\n",
    "#model.add(Dense(units=32, activation='relu'))\n",
    "#model.add(Dense(units=16, activation='relu'))\n",
    "# И активацию для скрытого слоя нейронов\n",
    "\n",
    "# Добавляем выходной полносвязный слой из 4 нейронов\n",
    "model.add(Dense(units=4, activation='softmax'))\n",
    "#model.add(Dense(units=1, activation='softmax'))\n",
    "\n",
    "# Чтобы получить на выходе вероятности для каждого класса, выбираем активацию\n",
    "# softmax\n",
    "model.summary()\n",
    "# Компилируем модель\n",
    "model.compile(loss='categorical_crossentropy',  # 'categorical_crossentropy' 'mean_squared_error'\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics='accuracy')  # [tf.keras.metrics.RootMeanSquaredError()]\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=1000, batch_size=34,\n",
    "                    validation_data=(X_test, y_test)\n",
    "                    )\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=1000, batch_size=34,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    )\n",
    "# early stopping\n",
    "\n",
    "from keras import callbacks\n",
    "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", min_delta=.001,\n",
    "                                        mode =\"min\", patience = 10, \n",
    "                                        restore_best_weights = True)\n",
    "  \n",
    "history = model.fit(X_train, y_train, batch_size = 201, \n",
    "                    epochs = 1000, validation_data =(X_val, y_val), \n",
    "                    callbacks =[earlystopping])\n",
    "hdf = pd.DataFrame(history.history)\n",
    "fig = px.line(hdf, x=hdf.index, y=['loss', 'val_loss', 'accuracy', 'val_accuracy'])  # 'root_mean_squared_error''loss', 'val_loss' 'accuracy', 'val_accuracy'\n",
    "fig.show()\n",
    "model.evaluate(X_test, y_test)\n",
    "# Метод predict модели возвращает выходные значения последнего слоя нейросети \n",
    "y_test_predictions = model.predict(X_test)\n",
    "y_predicted_classes=np.argmax(y_test_predictions, axis=1)\n",
    "y_real_classes=np.argmax(y_test, axis=1)\n",
    "y_true = encoder.inverse_transform(y_real_classes)\n",
    "y_pred = encoder.inverse_transform(y_predicted_classes)\n",
    "fig, ax = plt.subplots(figsize=(3,3), dpi=150)\n",
    "class_labels = ['Acetone', 'H2S', 'NO2', 'air']\n",
    "fig = ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize = 'pred', ax = ax, xticks_rotation=30, cmap = 'viridis')\n",
    "ax.set_title(f'NO2 (128-32) NN\\n accuracy = {model.evaluate(X_test, y_test)[1]:.2f}of')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a timesplit here befor aplying LSTM\n",
    "def create_lstm_dataset(data, labels, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(data)-look_back-1):\n",
    "        a = data[i:(i+look_back), :]  # take a batch of data - several rows\n",
    "        dataX.append(a)\n",
    "        dataY.append(labels[i + look_back - 1])  # predict the gas values of the last row taken\n",
    "    return np.array(dataX), np.array(dataY)  # returns an array of shape [samples, time steps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = create_lstm_dataset(X_train, y_train, look_back=look_back)\n",
    "testX, testY = create_lstm_dataset(X_test, y_test, look_back=look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, input_shape=(look_back, 402)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss=loss, optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape= (look_back, 402)))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss=loss, optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(trainX, trainY, epochs=200, batch_size=50, verbose=1, \n",
    "                    validation_data = (testX, testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = pd.DataFrame(history.history)\n",
    "fig = px.line(hdf, x=hdf.index, y=['loss', 'val_loss'])  # 'loss', 'val_loss' 'accuracy', 'val_accuracy'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "#trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "#testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = np.sqrt(mean_squared_error(trainY[0], trainPredict[:,0,:]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = np.sqrt(mean_squared_error(testY[0], testPredict[:,0,:]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Добавим слой Embedding ожидая на входе словарь размера 1000, и\n",
    "# на выходе вложение размерностью 64.\n",
    "#model.add(Embedding(input_dim=X_train.shape[-1], output_dim=32))\n",
    "\n",
    "look_back = 20\n",
    "# Добавим слой LSTM с 128 внутренними узлами.\n",
    "model.add(LSTM(units=64, activation='tanh', input_shape=(1,look_back)))\n",
    "#model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.express.colors import sample_colorscale\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "from sklearn.preprocessing import minmax_scale, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.signal import savgol_filter\n",
    "import optuna\n",
    "#import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Any\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.express.colors import sample_colorscale\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale, StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer, Normalizer, PowerTransformer, MaxAbsScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import PCA, KernelPCA, FastICA\n",
    "from sklearn.manifold import TSNE, Isomap, MDS, LocallyLinearEmbedding\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, mean_squared_error, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit, LeaveOneGroupOut, TimeSeriesSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from scipy.signal import savgol_filter, convolve\n",
    "from statsmodels.tsa.filters.filtertools import convolution_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool, CatBoost, CatBoostRegressor\n",
    "import shap\n",
    "import optuna\n",
    "from optuna.samplers import NSGAIISampler\n",
    "from optuna.integration import CatBoostPruningCallback\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_reader(path: str, format_: str = 'brotli') -> (list, list):\n",
    "    '''automatically detects if the path is long or short (starts with OneDrive/...)\n",
    "       if short, correctly extends the path to desired folder with C:/%username%/...\n",
    "       checks the orientation of slashes, changes it to \"/\" and adds \"/\" in the end\n",
    "       then returns two lists of files of the chosen format (.brotli, .xls) sorted by date\n",
    "       data_list = filenames\n",
    "       data_path_list = full paths\n",
    "    '''\n",
    "    path.replace('\\\\', '/')\n",
    "    if not path.endswith(\"/\"):\n",
    "        path += \"/\"\n",
    "    full_path = path if path.startswith(\"C:/\") else 'C:/Users/' + os.environ.get(\"USERNAME\") + '/' + path\n",
    "    data_list = [str(file)[len(full_path):] for file in\n",
    "                 sorted(Path(full_path).iterdir(), key=os.path.getmtime, reverse=True) if str(file).endswith(format_)]\n",
    "    data_path_list = [full_path + x for x in data_list]\n",
    "    data_list_df = pd.DataFrame(data=data_list,\n",
    "                                columns=['file names'],\n",
    "                                index=np.arange(len(data_list)))\n",
    "    print(data_list_df)\n",
    "    return data_list, data_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = r'YandexDisk\\SK_LNM\\zzz____LNM\\a_Article on SWCNT thermocycling\\Experimental data\\Feb 2021'\n",
    "data_list, data_path_list = folder_reader(path_to_data, '.brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(gas: str = 'NO2'):\n",
    "    gas_path_dict = {'NO2': r\"C:\\Users\\Bahrs\\YandexDisk\\SK_LNM\\zzz____LNM\\a_Article on SWCNT thermocycling\\Experimental data\\Feb 2021\\NO2_15_02_protocol10-15-25.brotli\",\n",
    "                     'H2S': r\"C:\\Users\\Bahrs\\YandexDisk\\SK_LNM\\zzz____LNM\\a_Article on SWCNT thermocycling\\Experimental data\\Feb 2021\\H2S_17_02_protocol10-15-25.brotli\",\n",
    "                     'Acet': r\"C:\\Users\\Bahrs\\YandexDisk\\SK_LNM\\zzz____LNM\\a_Article on SWCNT thermocycling\\Experimental data\\Feb 2021\\Acet_19_02_protocol10-15-25.brotli\",\n",
    "                     'NO2 ': r\"C:\\Users\\Bahrs\\YandexDisk\\SK_LNM\\zzz____LNM\\a_Article on SWCNT thermocycling\\Experimental data\\Feb 2021\\NO2_16_04_protocol10-15-25.brotli\",}\n",
    "    df = pd.read_parquet(gas_path_dict[gas])\n",
    "    if gas == 'H2S':\n",
    "        df = df[15*402:]\n",
    "    elif gas == 'NO2':\n",
    "        df = df[:-400]\n",
    "    elif gas == 'NO2 ':\n",
    "        df = df[:5*120*402]\n",
    "    else:\n",
    "        pass\n",
    "    switch_mask = (df['MFC_target'] == 0) & (df['MFC_target'].shift() == 25)\n",
    "    incremental_values = switch_mask.cumsum()\n",
    "    return df.assign(meas_cycle = incremental_values)\n",
    "def true_gas(df: pd.DataFrame):\n",
    "    #errors = df.flow_target_error.abs().values\n",
    "    return df.MFC_target.values - df.flow_target_error.values\n",
    "def SavGol(trace, window_length: int = 600, polyorder: int = 3) -> np.ndarray:\n",
    "    baseline_trend = savgol_filter(x = trace, window_length = window_length, polyorder = polyorder, mode = 'mirror')\n",
    "    return trace - baseline_trend\n",
    "def cut_into_Vpulses(data_column: np.array, gas_column: np.array, meas_cycle_column: np.array, gas: str, dp_per_pulse: int = 402): \n",
    "    \n",
    "    gas_column = gas_column.reshape((len(data_column)//dp_per_pulse, dp_per_pulse)).mean(axis=1)\n",
    "    meas_cycle = meas_cycle_column[::dp_per_pulse]\n",
    "    if len(data_column) % dp_per_pulse != 0:\n",
    "        print(f\"df shape is not divisible by {dp_per_pulse}\\n{len(data_column) % dp_per_pulse}\")\n",
    "    else:\n",
    "        datadf = data_column.reshape((len(data_column)//dp_per_pulse, dp_per_pulse))\n",
    "        zeros = np.zeros(len(data_column)//dp_per_pulse)\n",
    "        datadf = pd.DataFrame(datadf).assign(NO2 = zeros,\n",
    "                                             H2S = zeros,\n",
    "                                             Acet = zeros, \n",
    "                                             meas_cycle = meas_cycle)\n",
    "        datadf.loc[:,gas] = gas_column\n",
    "        return datadf.assign(gas=gas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_exponential_smoothing(data: np.ndarray, alpha: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply forward-backward exponential smoothing to data.\n",
    "\n",
    "    Parameters:\n",
    "        data (array_like): The input data to be smoothed.\n",
    "        alpha (float): Smoothing factor between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        smoothed_data (ndarray): Smoothed data.\n",
    "    \"\"\"\n",
    "    if alpha >=1 or alpha <= 0:\n",
    "        raise ValueError(\"Enter alpha such that 0 < alpha < 1\")\n",
    "    n = len(data)\n",
    "    smoothed_data = np.zeros_like(data, dtype=float)\n",
    "\n",
    "    # Forward pass\n",
    "    smoothed_data[0] = data[0]\n",
    "    for i in range(1, n):\n",
    "        smoothed_data[i] = alpha * data[i] + (1 - alpha) * smoothed_data[i - 1]\n",
    "\n",
    "    # Backward pass\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        smoothed_data[i] = alpha * smoothed_data[i] + (1 - alpha) * smoothed_data[i + 1]\n",
    "\n",
    "    return smoothed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedrift_n_cut(df, method: str, how: str, window_length: int, alpha: float = 0.015, gas: str = 'NO2'):\n",
    "    '''how - choose one of multienv, topenv, full to dedrift accordingly. Any other value will not dedrift data at all'''\n",
    "    data = df.I.values\n",
    "    gas_values = true_gas(df)\n",
    "    meas_cycle_column = df.meas_cycle.values\n",
    "    # dedrifting method\n",
    "    if method == 'SavGol':\n",
    "        filter_method = lambda x: savgol_filter(x=x, window_length = window_length, polyorder = 3, mode = 'mirror')\n",
    "    elif method == 'exp':\n",
    "        filter_method =  lambda x: forward_backward_exponential_smoothing(x, alpha)\n",
    "    elif method == 'none':\n",
    "        filter_method = lambda x: np.array([np.nanmean(x)]*len(x)).flatten()\n",
    "    else: \n",
    "        raise ValueError(f\"Currently only `exp` and `SavGol` methods of dedrifting are available. You entered {method}\")\n",
    "    \n",
    "    #envelope to correct\n",
    "    if how == 'topenv':\n",
    "        envelopes = [201]\n",
    "    elif how == 'multienv':\n",
    "        envelopes = [50, 150, 201, 300]\n",
    "    elif how == 'none':\n",
    "        envelopes = [201]\n",
    "    #elif how == 'full':  # to be avoided since it takes a lot of time\n",
    "    #    dedrifted = SavGol(data, window_length=window_length * 402)\n",
    "    else: \n",
    "        raise ValueError(f\"Currently only `topenv` and `multienv` (and `none`) envelope choices are available. You entered {how}\")\n",
    "    env_dict = {}\n",
    "    for envelope in envelopes:\n",
    "        env_dict[envelope] = filter_method(data[envelope::402])\n",
    "    envelope_ = np.array([[x]*402 for x in pd.DataFrame(env_dict).values.mean(axis=1)]).flatten()\n",
    "    dedrifted = data - envelope_\n",
    "    # cutting\n",
    "    return cut_into_Vpulses(dedrifted, gas_values, meas_cycle_column, gas, dp_per_pulse=402)\n",
    "\n",
    "def load_full_dedrifted_dataset(dedrifting_method: str = 'exp', envelope_choice: str = 'multienv', window_length: int = 350, alpha: float = 0.022):\n",
    "    '''dedrifting method - choose one of `SavGol`, `exp`, `None`\n",
    "    envelope_choice - choose one of `topenv`, `multienv`\n",
    "    window_length - be careful not to exceed the sample size'''\n",
    "    DF = pd.DataFrame()\n",
    "    for gas in ['NO2', 'H2S', 'Acet']:\n",
    "        df = dedrift_n_cut(load_data(gas), method=dedrifting_method, how=envelope_choice, window_length=window_length, alpha=alpha, gas=gas)\n",
    "        DF = pd.concat([DF, df], ignore_index=True)\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "#alpha = 0.2\n",
    "data = load_data(\"H2S\").I.values\n",
    "index = np.arange(len(data))\n",
    "fig.add_trace(go.Scattergl(x = np.arange(len(data)//6).tolist(), y = data[::6].tolist(), name=\"raw\", mode='lines'))\n",
    "for alpha in [0.01, 0.015, 0.02, 0.025, 0.03]:\n",
    "    envelope = np.array([[x]*402 for x in forward_backward_exponential_smoothing(data[201::402], alpha)]).flatten()\n",
    "    fig.add_trace(go.Scattergl(x = np.arange(len(data)//6).tolist(), y = envelope[::6].tolist(), name=f\"alpha = {alpha}\", mode='lines'))\n",
    "fig.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "alpha = 0.2\n",
    "data = load_data(\"NO2\").I.values\n",
    "index = np.arange(len(data))\n",
    "fig.add_trace(go.Scattergl(x=np.arange(len(data)//6).tolist(), y=data[::6].tolist(), name=\"raw\", mode='lines'))\n",
    "envelope = np.array([[x]*402 for x in forward_backward_exponential_smoothing(data[201::402], alpha)]).flatten()\n",
    "fig.add_trace(go.Scattergl(x = np.arange(len(data)/6), y = envelope[::6], name=f\"alpha = {alpha}\", mode='lines'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = load_full_dedrifted_dataset('SavGol', 'multienv', window_length=500, alpha=0.015)\n",
    "print(DF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.Acet.apply(lambda x: round(x,0)).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_weight(gas, look_back: int = 500, to_weight: bool = True):\n",
    "    '''function weights samples according to their TLV (the lower the TLV the higher the weight)\n",
    "    and according to the total count of samples in the respective measurement \n",
    "    currently cannot figure out how to weight samples by both. Left only TLV weighting\n",
    "    Almost figured that out, but somehow I constantly get 1-2% extra which is not good'''\n",
    "    gas_counts = DF.gas.value_counts().to_dict()\n",
    "    gas_tlvs = dict(NO2 = 1/0.2, H2S = 1/1, Acet = 1/250) if to_weight else dict(NO2=1, H2S=1, Acet=1) # TLV-TWA in ppm according to ACGIH\n",
    "    sum_counts = sum(gas_counts.values()) - look_back*len(gas_tlvs.keys())\n",
    "    sum_rec_TLV = sum(gas_tlvs.values())\n",
    "    return gas_tlvs[gas] / sum_rec_TLV  # * (gas_counts[gas] - look_back) / sum_counts * len(gas_tlvs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_n_PCA(train: np.array, test: np.array, n_components: int, scale: bool= True, scaler = MinMaxScaler(), do_PCA: bool = True):\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler() if scaler==None else scaler\n",
    "        train = scaler.fit_transform(train)\n",
    "        test = scaler.transform(test)\n",
    "    if do_PCA:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        train_pca = pca.fit_transform(train)\n",
    "        test_pca = pca.transform(test)\n",
    "        return train_pca, test_pca\n",
    "    else:\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_TS(df_: pd.DataFrame, n_components: int = 5, start: int = 1) -> (list, list, list, list):\n",
    "    '''start: int, meas_cycle to start with\n",
    "    returns 4 lists for train/test x/y. Each element = split'''\n",
    "    if not 'meas_cycle' in df_.columns:\n",
    "        print(\"wrong dataframe: no meas_cycle column\")\n",
    "        return None\n",
    "    else:\n",
    "        n_splits = len(df_.meas_cycle.unique())\n",
    "        train_X, test_X, train_y, test_y = [], [], [], []\n",
    "        for split in range(start,n_splits-1):  # starting from 1 cause the first meas cycle was cut\n",
    "            # PCA\n",
    "            train = df_.loc[df_['meas_cycle']<=split].iloc[:,:402].values\n",
    "            test = df_.loc[df_['meas_cycle']==split+1].iloc[:,:402].values\n",
    "            train_pca, test_pca = scale_n_PCA(train, test, n_components=n_components, scale=True, scaler=MinMaxScaler(), do_PCA=True)\n",
    "            train_X.append(train_pca)\n",
    "            train_y.append(df_.loc[df_['meas_cycle']<=split].iloc[:,402:405].values)\n",
    "            test_X.append(test_pca)\n",
    "            test_y.append(df_.loc[df_['meas_cycle']==split+1].iloc[:,402:405].values)\n",
    "        return train_X, test_X, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_TS_class(df_: pd.DataFrame, n_components: int = 5, start: int = 1) -> (list, list, list, list):\n",
    "    '''start: int, meas_cycle to start with\n",
    "    returns 4 lists for train/test x/y. Each element = split'''\n",
    "    if not ('meas_cycle' or 'class_') in df_.columns:\n",
    "        raise ValueError(\"wrong dataframe: no meas_cycle or class_ column\")\n",
    "    else:\n",
    "        n_splits = len(df_.meas_cycle.unique())\n",
    "        train_X, test_X, train_y, test_y = [], [], [], []\n",
    "        for split in range(start,n_splits-1):  # starting from 1 cause the first meas cycle was cut\n",
    "            # PCA\n",
    "            train = df_.loc[df_['meas_cycle']<=split].iloc[:,:402].values\n",
    "            test = df_.loc[df_['meas_cycle']==split+1].iloc[:,:402].values\n",
    "            train_pca, test_pca = scale_n_PCA(train, test, n_components=n_components, scale=True, scaler=MinMaxScaler(), do_PCA=True)\n",
    "            train_X.append(train_pca)\n",
    "            train_y.append(df_.loc[df_['meas_cycle']<=split].loc[:,\"class_\"].values)\n",
    "            test_X.append(test_pca)\n",
    "            test_y.append(df_.loc[df_['meas_cycle']==split+1].loc[:,\"class_\"].values)\n",
    "        return train_X, test_X, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_class_column(df: pd.DataFrame):\n",
    "    if not all(col in df.columns for col in ['NO2', 'H2S', 'Acet', 'gas']):\n",
    "        raise ValueError(\"DataFrame is missing required columns: 'NO2', 'H2S', 'Acet', 'gas'\")\n",
    "    \n",
    "    df['class_'] = df.apply(lambda row: 'air' if row['NO2'] + row['H2S'] + row['Acet'] == 0 else row['gas'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_sequences(data: np.ndarray, look_back: int = 50):\n",
    "    num_samples, num_features = data.shape\n",
    "    if num_samples//4*3 < look_back:\n",
    "        look_back = int(num_samples//4*3)\n",
    "        print(f\"Look back too large, number of samples is {num_samples}. Lookback is set to {look_back}\")\n",
    "    num_sequences = num_samples - look_back + 1\n",
    "    #print(num_sequences, look_back, num_features)\n",
    "    sequences = np.zeros((num_sequences, look_back, num_features))\n",
    "    for i in range(look_back):\n",
    "        sequences[:, i] = data[i:num_samples - look_back + 1 + i]\n",
    "    return list(sequences), look_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_sequences_for_multiple_gases(X: np.ndarray, y: np.ndarray, gas_type: np.ndarray, look_back: int = 50, to_weight: bool = True) -> (np.ndarray, np.ndarray):\n",
    "    df_ = pd.concat([pd.DataFrame(X), pd.DataFrame(y)], axis = 1, ignore_index=True).assign(gas_ = gas_type)\n",
    "    big_sequence_X, big_sequence_y = [], []\n",
    "    sample_weights = []\n",
    "    for gas in df_.gas_.unique():\n",
    "        sequence_X, new_look_back = create_RNN_sequences(data = df_.loc[df_['gas_'] == gas].iloc[:,:X.shape[1]].values, look_back=look_back)\n",
    "        sequence_y = df_.loc[df_['gas_']==gas].iloc[new_look_back - 1:, X.shape[1]:X.shape[1] + y.shape[1]].values\n",
    "        big_sequence_X.extend(list(sequence_X))\n",
    "        big_sequence_y.extend(list(sequence_y))\n",
    "        sample_weights.extend([sample_weight(gas, look_back = new_look_back, to_weight=to_weight)]*len(sequence_X))  # disable sample weiting\n",
    "    return np.array(big_sequence_X), np.array(big_sequence_y), np.array(sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_RNN(df_: pd.DataFrame, look_back: int = 50, n_components: int = 5, do_PCA: bool = True, to_weight: bool = False, start: int = 7) -> (list, list, list, list):\n",
    "    '''\n",
    "    returns 6 lists for train/test/weights x/y. x and y are splitted by meas_cycle 7'''\n",
    "    if not 'meas_cycle' in df_.columns:  # вставить защиту от дурака и слишком длинного look_back\n",
    "        print(\"wrong dataframe: no meas_cycle column\")\n",
    "        return None\n",
    "    else:\n",
    "        #do_PCA_ = True if do_PCA == 'yes' else False\n",
    "        #n_splits = len(df_.meas_cycle.unique())\n",
    "        train_X, test_X, train_y, test_y = [], [], [], []\n",
    "        train_y_ = df_.loc[df_['meas_cycle']<=start].iloc[:,402:405].values\n",
    "        test_y_ = df_.loc[df_['meas_cycle']>start].iloc[:,402:405].values  # df_['meas_cycle']==split+1\n",
    "        train_gas = df_.loc[df_['meas_cycle']<=start].loc[:, 'gas'].values\n",
    "        test_gas = df_.loc[df_['meas_cycle']>start].loc[:, 'gas'].values  # df_['meas_cycle']==split+1\n",
    "\n",
    "        # PCA\n",
    "        train = df_.loc[df_['meas_cycle']<=start].iloc[:,:402].values\n",
    "        test = df_.loc[df_['meas_cycle']>start].iloc[:,:402].values  # df_['meas_cycle']==split+1\n",
    "        train_pca, test_pca = scale_n_PCA(train, test, n_components=n_components, scale=True, scaler=MinMaxScaler(), do_PCA=do_PCA)\n",
    "\n",
    "        # careful splitting\n",
    "        train_X_, train_y__, train_sw = create_RNN_sequences_for_multiple_gases(X = train_pca, y = train_y_, gas_type = train_gas, look_back = look_back, to_weight=to_weight)\n",
    "        train_X.append(train_X_)\n",
    "        train_y.append(train_y__)\n",
    "        \n",
    "        test_X_, test_y__, test_sw = create_RNN_sequences_for_multiple_gases(X = test_pca, y = test_y_, gas_type = test_gas, look_back = look_back, to_weight=to_weight)\n",
    "        test_X.append(test_X_)\n",
    "        test_y.append(test_y__)\n",
    "\n",
    "        return train_X, test_X, train_y, test_y, train_sw, test_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_RNN_(df_: pd.DataFrame, look_back: int = 50, n_components: int = 5, start: int = 5, do_PCA: bool = True, to_weight: bool = False) -> (list, list, list, list):\n",
    "    '''\n",
    "    returns 4 lists for train/test x/y. Each element = split\n",
    "    start - measurement cycle to start with (each set of training data contains one more meas_cycle)'''\n",
    "    if not 'meas_cycle' in df_.columns:  # вставить защиту от дурака и слишком длинного look_back\n",
    "        print(\"wrong dataframe: no meas_cycle column\")\n",
    "        return None\n",
    "    else:\n",
    "        #do_PCA_ = True if do_PCA == 'yes' else False\n",
    "        n_splits = len(df_.meas_cycle.unique())\n",
    "        train_X, test_X, train_y, test_y = [], [], [], []\n",
    "        for split in range(start,n_splits-1):  # starting from 1 cause the first meas cycle was cut\n",
    "            train_y_ = df_.loc[df_['meas_cycle']<=split].iloc[:,402:405].values\n",
    "            test_y_ = df_.loc[df_['meas_cycle']==split+1].iloc[:,402:405].values  # df_['meas_cycle']==split+1  df_['meas_cycle']>split\n",
    "            train_gas = df_.loc[df_['meas_cycle']<=split].loc[:, 'gas'].values\n",
    "            test_gas = df_.loc[df_['meas_cycle']==split+1].loc[:, 'gas'].values  # df_['meas_cycle']==split+1\n",
    "\n",
    "            # PCA\n",
    "            train = df_.loc[df_['meas_cycle']<=split].iloc[:,:402].values\n",
    "            test = df_.loc[df_['meas_cycle']>split].iloc[:,:402].values  # df_['meas_cycle']==split+1\n",
    "            train_pca, test_pca = scale_n_PCA(train, test, n_components=n_components, scale=True, scaler=MinMaxScaler(), do_PCA=do_PCA)\n",
    "\n",
    "            # careful splitting\n",
    "            train_X_, train_y__, train_sw = create_RNN_sequences_for_multiple_gases(X = train_pca, y = train_y_, gas_type = train_gas, look_back = look_back, to_weight=to_weight)\n",
    "            train_X.append(train_X_)\n",
    "            train_y.append(train_y__)\n",
    "            \n",
    "            test_X_, test_y__, test_sw = create_RNN_sequences_for_multiple_gases(X = test_pca, y = test_y_, gas_type = test_gas, look_back = look_back, to_weight=to_weight)\n",
    "            test_X.append(test_X_)\n",
    "            test_y.append(test_y__)\n",
    "\n",
    "        return train_X, test_X, train_y, test_y, train_sw, test_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, params: dict) -> None:\n",
    "    '''plots the history of NN training and saves it to a folder'''\n",
    "    fig = px.line(pd.DataFrame(history).applymap(np.sqrt))\n",
    "    #fig_title = str(f'{params}')\n",
    "    annotations_keys = [\n",
    "        dict(\n",
    "            x=0.95,  # Adjust the x-coordinate for alignment\n",
    "            y=0.88 - i * 0.06,  # Adjust the y-coordinate for vertical spacing\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            text=str(key),\n",
    "            showarrow=False,\n",
    "            font_size=11,)\n",
    "        for i, key in enumerate(params.keys())\n",
    "    ]\n",
    "    annotations_values = [dict(x=1,  # Adjust the x-coordinate for alignment\n",
    "            y=0.88 - i * 0.06,  # Adjust the y-coordinate for vertical spacing\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            text=str(value) if type(value) != float else str(round(value, 5)),\n",
    "            showarrow=False,\n",
    "            font_size=11)\n",
    "        for i, value in enumerate(params.values())]\n",
    "    fig.update_layout(xaxis_title = 'Epoch #',\n",
    "                    yaxis_title = 'RMSE, ppm',\n",
    "                    legend_title = '',\n",
    "                    font_size = 20,\n",
    "                    annotations = annotations_keys + annotations_values,\n",
    "                    #title_text = fig_title,\n",
    "                    #title_font_size = 9,\n",
    "                    legend_orientation = 'h',\n",
    "                    legend_x = 1,\n",
    "                    legend_y = 1,\n",
    "                    legend_xanchor = 'right',\n",
    "                    legend_yanchor = 'top',\n",
    "                    yaxis_nticks = 5,\n",
    "                    margin = dict(t=30, b=0, r=0, l=10),\n",
    "                    width = 1500,\n",
    "                    height = 400)\n",
    "    savepath = r\"C:\\Users\\Bahrs\\YandexDisk\\SK_LNM\\zzz____LNM\\a_Article on SWCNT thermocycling\\Experimental data\\Feb 2021\\NN training graphs\".replace('\\\\', '/')\n",
    "    savepath += \"/\" + \"_\".join([str(x) for x in params.values()]) + \".png\"\n",
    "    fig.write_image(savepath, format = 'png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_CM(model, X, y, normalize, text: str = None):\n",
    "    fig, ax = plt.subplots(figsize=(3,3), dpi=150)\n",
    "    labels = [r\"$acetone$\", r'$H_{2}S$', r'$NO_{2}$', r'$air$']\n",
    "    fig = ConfusionMatrixDisplay.from_estimator(estimator = model, X = X, y = y, normalize = normalize, xticks_rotation='horizontal', \n",
    "                                                ax = ax, cmap = 'viridis',\n",
    "                                                display_labels=labels,\n",
    "                                                )\n",
    "\n",
    "    # Set title with F1 score and number of measurement cycles\n",
    "    predictions = model.predict(X)\n",
    "    f1_macro = f1_score(y, predictions, average='macro')\n",
    "    title = f\"Confusion Matrix \\nF1 Score: {f1_macro:.2f}\\n{text}\"\n",
    "\n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.set_xticklabels(labels, fontsize=8)\n",
    "    ax.set_yticklabels(labels, fontsize=8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y, train_SW, test_SW = train_test_RNN(DF, look_back = 50, n_components=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def make_LSTM(X, y, n_LSTM_layers: int = 1, n_units: int = 64, dropout: float = 0, optimizer: tf.keras.optimizers = Adam, learning_rate: float = 0.001, loss: str = 'mean_squared_error'):\n",
    "    input_shape = ((X[0]).shape[1], X[0].shape[2])\n",
    "    output_shape = y[0].shape[1]\n",
    "    to_return_sequences = lambda n_LSTM_layers_left: True if n_LSTM_layers_left > 1 else False\n",
    "\n",
    "    model = Sequential()\n",
    "    for n_LSTM_layers_left in range(n_LSTM_layers, 0, -1):\n",
    "        model.add(LSTM(units=n_units, \n",
    "                       return_sequences=to_return_sequences(n_LSTM_layers_left), \n",
    "                       input_shape=input_shape, \n",
    "                       dropout = dropout, \n",
    "                       recurrent_dropout=dropout))\n",
    "        n_units = n_units // 2 if n_units > 8 else n_units\n",
    "    model.add(Dense(units=output_shape))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer(learning_rate=learning_rate), weighted_metrics = [])\n",
    "\n",
    "    return model\n",
    "\n",
    "def fit_LSTM(model, X_train, y_train, X_test, y_test, sw_train, sw_test, epochs: int = 100, batch_size: int = 32, return_history: bool = False):\n",
    "    patience = 30\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=patience, min_delta=0.1, restore_best_weights=True)]\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data = (X_test, y_test),\n",
    "        #validation_data=(X_test, y_test, sw_test),\n",
    "        #sample_weight = sw_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=0,\n",
    "        callbacks = callbacks,\n",
    "        shuffle = False,\n",
    "        #workers = 32,\n",
    "        #use_multiprocessing = True\n",
    "        )\n",
    "    #test_loss = model.evaluate(X_test, y_test, sample_weight = sw_test, batch_size = batch_size)\n",
    "    y_pred = model.predict(X_test, batch_size = batch_size, verbose = 0,\n",
    "                           #workers = 16, use_multiprocessing = True,\n",
    "                           )\n",
    "    test_rmse = mean_squared_error(y_test, y_pred, squared = False, multioutput = 'uniform_average')\n",
    "    #test_rmse = mean_squared_error(y_test, y_pred, sample_weight= sw_test, squared = False, multioutput = 'uniform_average')\n",
    "    ### taking history into account and trying to minimize the gap between train and validation loss\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    mse_epochs = [(x-y)**2 for x, y in zip(train_loss, val_loss)]\n",
    "    history_slice = slice(-patience,len(mse_epochs),1)# slice(-3*patience//2,-patience//2,1)  # apparently it doesn't bug out if len(mse_epochs) < 2* patience\n",
    "    historical_loss_rmse = np.sqrt(np.nanmean(mse_epochs[history_slice]))  # this slice was chosen as it contains the most optimized hyperparameters\n",
    "    # here a new metric - historical_loss_rmse was introduced, that shows how far validation and training loss at the last patience epochs are\n",
    "\n",
    "    hist_test_weights = [10,0]  # completely arbitrary parameter that sets up a weighted sum of traditional RMSE and HLRMSE with weights 2 and 1 respectively\n",
    "    test_loss = (np.array([test_rmse, historical_loss_rmse]) * np.array(hist_test_weights)).sum() / sum(hist_test_weights)\n",
    "    if return_history:\n",
    "        return test_loss, history.history\n",
    "    else:\n",
    "        return test_loss, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    dedrifting_method = 'SavGol',\n",
    "    envelope_choice = 'multienv',  # trial.suggest_categorical('dedrifting_method', ['multienv', 'topenv', 'None']),  #'multienv',  # \n",
    "    window_length = 250, #trial.suggest_int('window_length', 100, 700, step = 10),\n",
    "    alpha = 0.022,\n",
    "    look_back = 35, #trial.suggest_int('look_back', 5, 250),\n",
    "    n_components = 110, # trial.suggest_int('n_components', 2, 70),\n",
    "    do_PCA = True, # trial.suggest_categorical('do_PCA', [True, False]),\n",
    "    n_LSTM_layers = 1, #trial.suggest_int('n_LSTM_layers', 1, 3),\n",
    "    n_units = 35, # trial.suggest_int('n_units', 4, 256, log=True),\n",
    "    dropout = 0.2, # trial.suggest_float('dropout', 0.05, 0.5, log=True),\n",
    "    learning_rate= 0.02, #trial.suggest_float('learning_rate', 0.0001, 0.1, log=True),\n",
    "    epochs = 200, #trial.suggest_int('epochs', 50, 300),\n",
    "    batch_size = 128, #trial.suggest_categorical('batch_size', [8, 16, 32, 64, 128, 256]),\n",
    "    )\n",
    "DF = load_full_dedrifted_dataset(dedrifting_method=params['dedrifting_method'], envelope_choice=params['envelope_choice'], window_length=params['window_length'])\n",
    "# Split data and perform PCA\n",
    "train_X, test_X, train_y, test_y, train_SW, test_SW = train_test_RNN(DF, \n",
    "                                                    look_back= params['look_back'],\n",
    "                                                    n_components= params['n_components'],\n",
    "                                                    do_PCA= params['do_PCA'])\n",
    "model = make_LSTM(train_X, \n",
    "                    train_y, \n",
    "                    optimizer = Adam, \n",
    "                    loss = 'mean_squared_error',\n",
    "                    n_LSTM_layers = params['n_LSTM_layers'],\n",
    "                    n_units = params['n_units'],\n",
    "                    dropout = params['dropout'],\n",
    "                    learning_rate = params['learning_rate'],)\n",
    "# train the model\n",
    "mse = []\n",
    "#for X_train, X_test, y_train, y_test in zip(train_X[::-1], test_X[::-1], train_y[::-1], test_y[::-1]):  # reversed the order so the the trial be pruned earlier and more consistent\n",
    "mse_, history_ = fit_LSTM(\n",
    "    model, train_X[0], train_y[0], test_X[0], test_y[0], train_SW, test_SW,\n",
    "    epochs = params['epochs'],\n",
    "    batch_size= params['batch_size'], \n",
    "    return_history=True)\n",
    "plot_history(history_, params)\n",
    "#mse.append(mse_) \n",
    "print(mse_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_X[0], batch_size = params['batch_size'], \n",
    "                           #workers = 16, use_multiprocessing = True,\n",
    "                           )\n",
    "test_rmse = mean_squared_error(test_y[0], y_pred, sample_weight= test_SW, squared = False, multioutput = 'raw_values')\n",
    "result_df = pd.DataFrame(data = test_rmse, index = DF.columns[402:405], columns = [\"RMSE\"]).T\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.line(pd.DataFrame(history_).applymap(np.sqrt))\n",
    "annotations_keys = [\n",
    "    dict(\n",
    "        x=0.95,  # Adjust the x-coordinate for alignment\n",
    "        y=0.88 - i * 0.06,  # Adjust the y-coordinate for vertical spacing\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        text=str(key),\n",
    "        showarrow=False,\n",
    "        font_size=11,)\n",
    "    for i, key in enumerate(params.keys())\n",
    "]\n",
    "annotations_values = [dict(x=1,  # Adjust the x-coordinate for alignment\n",
    "         y=0.88 - i * 0.06,  # Adjust the y-coordinate for vertical spacing\n",
    "         xref=\"paper\",\n",
    "         yref=\"paper\",\n",
    "         text=str(value) if type(value) != float else str(round(value, 5)),\n",
    "         showarrow=False,\n",
    "         font_size=11)\n",
    "    for i, value in enumerate(params.values())]\n",
    "\n",
    "fig.update_layout(xaxis_title = 'Epoch #',\n",
    "                  yaxis_title = 'RMSE, ppm',\n",
    "                  legend_title = '',\n",
    "                  annotations = annotations_keys + annotations_values,\n",
    "                  font_size = 20,\n",
    "                  legend_orientation = 'h',\n",
    "                  legend_x = 1,\n",
    "                  legend_y = 1,\n",
    "                  legend_xanchor = 'right',\n",
    "                  legend_yanchor = 'top',\n",
    "                  yaxis_nticks = 5,\n",
    "                  margin = dict(t=30, b=0, r=0, l=10),\n",
    "                  width = 1500,\n",
    "                  height = 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_TS_class(add_class_column(DF), n_components = 153, start = 4)\n",
    "new_NO2 = add_class_column(dedrift_n_cut(load_data(\"NO2 \"), 'exp', 'multienv', 285, alpha= 0.02172, gas=\"NO2\"))\n",
    "ntrain_X, ntest_X, ntrain_y, ntest_y = train_test_TS_class(new_NO2, n_components = 153, start = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'dedrifting_method': 'exp',\n",
    " 'window_length': 285,\n",
    " 'alpha': 0.02171655699670033,\n",
    " 'n_components': 153,\n",
    " 'iterations': 1300,\n",
    " 'depth': 6,\n",
    " 'learning_rate': 0.009372428573829332,\n",
    " 'l2_leaf_reg': 13.155579412870834}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    iterations= 1300,\n",
    "    depth=6,\n",
    "    learning_rate=0.00937,\n",
    "    l2_leaf_reg=13,\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"Accuracy\",\n",
    ")\n",
    "f1_macro_scores = []\n",
    "for i in range(len(train_X)):\n",
    "    model.fit(\n",
    "        train_X[i], train_y[i], \n",
    "        eval_set = (test_X[i], test_y[i]), \n",
    "        use_best_model=True,\n",
    "        silent=True,\n",
    "        plot=False)\n",
    "    \n",
    "    predictions = model.predict(test_X[i])\n",
    "    f1_macro_scores.append(f1_score(test_y[i], predictions, average='macro'))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(3,3), dpi=150)\n",
    "    labels = [r\"$acetone$\", r'$H_{2}S$', r'$NO_{2}$', r'$air$']\n",
    "    fig = ConfusionMatrixDisplay.from_estimator(estimator = model, X = test_X[i], y = test_y[i], normalize = 'pred', xticks_rotation='horizontal', \n",
    "                                                ax = ax, cmap = 'viridis',\n",
    "                                                display_labels=labels)\n",
    "    \n",
    "    # Set title with F1 score and number of measurement cycles\n",
    "    title = f\"Confusion Matrix \\nF1 Score: {f1_score(test_y[i], predictions, average='macro'):.2f}\\n meas_cycle {(5+i)}\"\n",
    "    \n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.set_xticklabels(labels, fontsize=8)\n",
    "    ax.set_yticklabels(labels, fontsize=8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain_X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((ntrain_X[0], train_X[-1])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    iterations= 500,\n",
    "    depth=6,\n",
    "    learning_rate=0.014,\n",
    "    l2_leaf_reg=4,\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"Accuracy\",\n",
    ")\n",
    "f1_macro_scores = []\n",
    "model.fit(\n",
    "    train_X[-1], train_y[-1], \n",
    "    eval_set = (test_X[-1], test_y[-1]), \n",
    "    use_best_model=True,\n",
    "    silent=True,\n",
    "    plot=False)\n",
    "\n",
    "predictions = model.predict(np.concatenate((test_X[-1],ntrain_X[0])))\n",
    "f1_macro_scores.append(f1_score(np.concatenate((test_y[-1],ntrain_y[0])), predictions, average='macro'))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3,3), dpi=150)\n",
    "labels = [r\"$acetone$\", r'$H_{2}S$', r'$NO_{2}$', r'$air$']\n",
    "fig = ConfusionMatrixDisplay.from_estimator(estimator = model, X = np.concatenate((test_X[-1],ntrain_X[0])), y = np.concatenate((test_y[-1],ntrain_y[0])), normalize = 'pred', xticks_rotation='horizontal', \n",
    "                                            ax = ax, cmap = 'viridis',\n",
    "                                            display_labels=labels)\n",
    "\n",
    "# Set title with F1 score and number of measurement cycles\n",
    "title = f\"Confusion Matrix \\nF1 Score: {f1_score(np.concatenate((test_y[-1],ntrain_y[0])), predictions, average='macro'):.2f}\\n meas_cycle {(5+i)}\"\n",
    "\n",
    "ax.set_title(title, fontsize=10)\n",
    "ax.set_xticklabels(labels, fontsize=8)\n",
    "ax.set_yticklabels(labels, fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Optuna functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_LSTM(trial):\n",
    "    # Load_data\n",
    "    data_load_params = dict(\n",
    "        dedrifting_method = trial.suggest_categorical('dedrifting_method', ['SavGol', 'exp', \"none\"]), # 'SavGol', \n",
    "        )\n",
    "    if data_load_params['dedrifting_method'] == 'SavGol':\n",
    "        data_load_params['window_length'] = trial.suggest_int('window_length', 150, 500, step = 10)\n",
    "        data_load_params['envelope_choice'] = trial.suggest_categorical('envelope_choice', ['multienv', 'topenv'])\n",
    "        data_load_params['alpha'] = 1\n",
    "    elif data_load_params['dedrifting_method'] == 'exp':\n",
    "        data_load_params['alpha'] = trial.suggest_float('alpha', 0.001, 0.1, log = True)\n",
    "        data_load_params['window_length'] = 1\n",
    "        data_load_params['envelope_choice'] = trial.suggest_categorical('envelope_choice', ['multienv', 'topenv'])\n",
    "    elif data_load_params['dedrifting_method'] == 'none':\n",
    "        data_load_params['envelope_choice'] = 'none'\n",
    "        data_load_params['window_length'] = 1\n",
    "        data_load_params['alpha'] = 1\n",
    "    \n",
    "    params = dict(\n",
    "        look_back = trial.suggest_int('look_back', 20, 57, log=False),\n",
    "        n_components = trial.suggest_int('n_components', 15, 150),\n",
    "        do_PCA = True,  # trial.suggest_categorical('do_PCA', [True, False]),\n",
    "        n_LSTM_layers = 1,  # trial.suggest_int('n_LSTM_layers', 1, 2),\n",
    "        n_units = trial.suggest_int('n_units', 16, 128, log=False),  # trial.suggest_categorical('n_units', [16, 32, 64, 96, 128]),  \n",
    "        dropout = trial.suggest_float('dropout', 0.005, 0.5, log=False),  # 0.162\n",
    "        learning_rate= trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        epochs = 150,  # trial.suggest_int('epochs', 50, 300),\n",
    "        batch_size = trial.suggest_categorical('batch_size', [64, 128, 256]),\n",
    "        )\n",
    "    \n",
    "\n",
    "    DF = load_full_dedrifted_dataset(**data_load_params)\n",
    "    # Split data and perform PCA\n",
    "    train_X, test_X, train_y, test_y, train_SW, test_SW = train_test_RNN(\n",
    "        DF, \n",
    "        look_back= params['look_back'],\n",
    "        n_components= params['n_components'],\n",
    "        do_PCA= params['do_PCA'],\n",
    "        start=8)\n",
    "    model = make_LSTM(train_X, \n",
    "                      train_y, \n",
    "                      optimizer = Adam, \n",
    "                      loss = 'mean_squared_error',\n",
    "                      n_LSTM_layers = params['n_LSTM_layers'],\n",
    "                      n_units = params['n_units'],\n",
    "                      dropout = params['dropout'],\n",
    "                      learning_rate = params['learning_rate'],)\n",
    "    # train the model\n",
    "    #mse = []\n",
    "    #pruning_flag = 0\n",
    "    #for X_train, X_test, y_train, y_test in zip(train_X[::-1], test_X[::-1], train_y[::-1], test_y[::-1]):  # reversed the order so the the trial be pruned earlier and more consistent\n",
    "    rmse_, history_ = fit_LSTM(\n",
    "        model, train_X[0], train_y[0], test_X[0], test_y[0], \n",
    "        train_SW, test_SW,\n",
    "        epochs = params['epochs'],\n",
    "        batch_size= params['batch_size'], \n",
    "        return_history=True)\n",
    "    plot_history(history_, params)\n",
    "    \n",
    "    return rmse_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_CBR(trial):\n",
    "    # Load_data\n",
    "    data_load_params = dict(dedrifting_method= trial.suggest_categorical('dedrifting_method', ['exp', 'none']),  # 'SavGol', \n",
    "                            envelope_choice = trial.suggest_categorical('envelope_choice', ['multienv', 'topenv']),\n",
    "                            )\n",
    "    if data_load_params['dedrifting_method'] == 'SavGol':\n",
    "        data_load_params['window_length'] = trial.suggest_int('window_length', 150, 400, step = 10)\n",
    "    elif data_load_params['dedrifting_method'] == 'exp':\n",
    "        data_load_params['alpha'] = trial.suggest_float('alpha', 0.001, 0.1, log = True)\n",
    "    \n",
    "    DF = load_full_dedrifted_dataset(**data_load_params)\n",
    "    # Split data and perform PCA\n",
    "    train_X, test_X, train_y, test_y = train_test_TS(DF, n_components=trial.suggest_int('n_components', 5, 150), start = 6)\n",
    "    # train the model\n",
    "    cb_params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1800, step = 50),\n",
    "        'depth': trial.suggest_int('depth', 3, 7),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.5, log = True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 3, 25)\n",
    "    }\n",
    "    catboost_model = CatBoostRegressor(**cb_params, loss_function='MultiRMSE', verbose=0)\n",
    "    pruning_callback = CatBoostPruningCallback(trial, \"MultiRMSE\")\n",
    "    mse = []\n",
    "    #pruning_flag = 0\n",
    "    for X_train, X_test, y_train, y_test in zip(train_X[::-1], test_X[::-1], train_y[::-1], test_y[::-1]):  # reversed the order so the the trial be pruned earlier and more consistent\n",
    "        catboost_model.fit(X_train, y_train, \n",
    "                           eval_set=[(X_test, y_test)], \n",
    "                           early_stopping_rounds=200, # need to implement good pruning by starting from the most information train|test pair and not calculate the whole sequence if this pair is pruned\n",
    "                           callbacks=[pruning_callback],\n",
    "                           verbose=0)\n",
    "        pruning_callback.check_pruned()\n",
    "        y_pred = catboost_model.predict(X_test)  # add native penalization (sample_weights) according to TLV_TWA values for gases, NO2 - 0.2, H2S - 1, Acet - \n",
    "        mse_ = mean_squared_error(y_test, y_pred, multioutput='raw_values', squared = False) # substitute this with native CatBoost stuff\n",
    "        TLV_weighted_mean = np.mean([x/y for x, y in zip(mse_,[0.2, 1, 250])])  # Assuming column order to be NO2 H2S Acetone\n",
    "        mse.append(TLV_weighted_mean) \n",
    "    return np.mean(mse_)  # np.mean(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_CBC(trial):\n",
    "    # Load_data\n",
    "    data_load_params = dict(dedrifting_method= trial.suggest_categorical('dedrifting_method', ['SavGol', 'exp', None]),\n",
    "                            envelope_choice = trial.suggest_categorical('envelope_choice', ['multienv', 'topenv']),\n",
    "                            )\n",
    "    if data_load_params['dedrifting_method'] == 'SavGol':\n",
    "        data_load_params['window_length'] = trial.suggest_int('window_length', 150, 400, step = 10)\n",
    "        data_load_params['alpha'] = 1\n",
    "    elif data_load_params['dedrifting_method'] == 'exp':\n",
    "        data_load_params['alpha'] = trial.suggest_float('alpha', 0.001, 0.1, log = True)\n",
    "    DF = load_full_dedrifted_dataset(**data_load_params)\n",
    "    # Split data and perform PCA\n",
    "    train_X, test_X, train_y, test_y = train_test_TS_class(add_class_column(DF), trial.suggest_int('n_components', 5, 200), start = 7)\n",
    "    # train the model\n",
    "    cb_params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 2000, step = 50),\n",
    "        'depth': trial.suggest_int('depth', 3, 7),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.7, log = True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 3, 15)\n",
    "    }\n",
    "    \n",
    "    catboost_model = CatBoostClassifier(**cb_params, loss_function=\"MultiClass\", eval_metric=\"Accuracy\", use_best_model=True, silent=True)\n",
    "    pruning_callback = CatBoostPruningCallback(trial, \"MultiClass\")\n",
    "    mse = []\n",
    "    #pruning_flag = 0\n",
    "    f1_macro_scores = []\n",
    "    for X_train, X_test, y_train, y_test in zip(train_X[::-1], test_X[::-1], train_y[::-1], test_y[::-1]):  # reversed the order so the the trial be pruned earlier and more consistent\n",
    "        catboost_model.fit(X_train, y_train, \n",
    "                           eval_set=[(X_test, y_test)], \n",
    "                           early_stopping_rounds=200, # need to implement good pruning by starting from the most information train|test pair and not calculate the whole sequence if this pair is pruned\n",
    "                           callbacks=[pruning_callback],\n",
    "                           verbose=0)\n",
    "        pruning_callback.check_pruned()\n",
    "        '''if not pruning_flag:\n",
    "            pruning_callback.check_pruned()\n",
    "            pruning_flag += 1\n",
    "            if trial.should_prune():\n",
    "                break'''\n",
    "        y_pred = catboost_model.predict(X_test)  # add native penalization (sample_weights) according to TLV_TWA values for gases, NO2 - 0.2, H2S - 1, Acet - \n",
    "        f1_macro_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    \n",
    "    return np.mean(f1_macro_scores)  # np.mean(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import optuna\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "study = optuna.create_study(direction= 'minimize',  # minimize  for regression minimize  for classification 'maximize'\n",
    "                            pruner= optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=0, interval_steps=1, n_min_trials=1),\n",
    "                            #optuna.pruners.SuccessiveHalvingPruner(min_resource='auto', reduction_factor=3, min_early_stopping_rate=4, bootstrap_count=0) \n",
    "                            # optuna.pruners.MedianPruner(n_warmup_steps=10),\n",
    "                            )\n",
    "study.optimize(objective_LSTM, n_trials=10000, timeout=8*60*60, show_progress_bar = False, n_jobs=-1)  #n_jobs=-1\n",
    "\n",
    "# Print the optimized hyperparameters and their values\n",
    "print('Number of finished trials: ', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Value: ', trial.value)\n",
    "print('Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how the model with the best hyperparameters performs\n",
    "best_params = study.best_params\n",
    "DF = load_full_dedrifted_dataset(best_params['dedrifting_method'], best_params['envelope_choice'], best_params['window_length'], best_params['alpha'])  # \n",
    "train_X, test_X, train_y, test_y = train_test(DF, n_components=best_params['n_components'], start = 5)\n",
    "catboost_model = CatBoostRegressor(iterations = best_params['iterations'], \n",
    "                                   depth = best_params['depth'],\n",
    "                                   learning_rate = best_params['learning_rate'],\n",
    "                                   loss_function='MultiRMSE', verbose=0)\n",
    "mse = []\n",
    "for X_train, X_test, y_train, y_test in zip(train_X, test_X, train_y, test_y):\n",
    "    catboost_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0)\n",
    "    y_pred = catboost_model.predict(X_test)\n",
    "    mse.append(mean_squared_error(y_test, y_pred, multioutput='raw_values', squared = False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "best_params = dict(\n",
    "        envelope_choice = 'multienv',\n",
    "        dedrifting_method = 'exp',\n",
    "        window_length = 350,\n",
    "        alpha = 0.022,\n",
    "        look_back = 109,\n",
    "        n_components = 98,\n",
    "        do_PCA = True, \n",
    "        n_LSTM_layers = 1,\n",
    "        n_units = 87, \n",
    "        dropout = 0.095538, \n",
    "        learning_rate= 0.018, \n",
    "        epochs = 120, \n",
    "        batch_size = 16,)\n",
    "best_params = best_params | study.best_params\n",
    "DF = load_full_dedrifted_dataset(dedrifting_method=best_params['dedrifting_method'], envelope_choice=best_params['envelope_choice'], window_length=best_params['window_length'], alpha= best_params['alpha'])\n",
    "train_X, test_X, train_y, test_y, train_SW, test_SW = train_test_RNN(DF, look_back= best_params['look_back'], n_components= best_params['n_components'], do_PCA= best_params['do_PCA'])\n",
    "model = make_LSTM(train_X, train_y, optimizer = Adam, loss = 'mean_squared_error', n_LSTM_layers = best_params['n_LSTM_layers'], n_units = best_params['n_units'], dropout = best_params['dropout'],\n",
    "                  learning_rate = best_params['learning_rate'],)\n",
    "rmse_, history_ = fit_LSTM(model, train_X[0], train_y[0], test_X[0], test_y[0], train_SW, test_SW,\n",
    "    epochs = best_params['epochs'], batch_size= best_params['batch_size'], return_history=True)\n",
    "plot_history(history_, best_params)\n",
    "print(rmse_)\n",
    "y_pred = model.predict(test_X[0], batch_size = best_params['batch_size'], workers = 16, use_multiprocessing = True)\n",
    "test_rmse = mean_squared_error(test_y[0], y_pred, sample_weight= test_SW, squared = False, multioutput = 'raw_values')\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_X[0], test_y[0], batch_size = best_params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_, best_params | dict(a=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)\n",
    "pd.DataFrame(mse, index = np.arange(6,10), columns = 'NO2 H2S Acet'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study).update_layout(height = 300, margin = dict(t=50, l=20, r=10, b=20)).show()\n",
    "optuna.visualization.plot_param_importances(study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\").update_layout(height = 300, \n",
    "                                                                                                                                      margin = dict(t=50, l=20, r=10, b=20)).show()\n",
    "optuna.visualization.plot_slice(study).show()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_rank(study, params = ['look_back', 'window_length'])  # , plot_contour params=[\"n_LSTM_layers\", \"n_units\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_slice(study)\n",
    "fig.update_layout(margin = dict(t=50, l=50, r=10, b=20))\n",
    "format_ = 'svg'\n",
    "slice_savepath = r\"C:\\Users\\Bahrs\\YandexDisk\\SK_LNM\\zzz____LNM\\a_Article on SWCNT thermocycling\\Experimental data\".replace(\"\\\\\",\"/\")\n",
    "slice_savepath += \"/\" + \"look_back_optimization_5_324\" + \".\" + format_\n",
    "fig.write_image(slice_savepath, format = format_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.trials_dataframe().to_csv(\"Optuna.25.09.csv\", sep = '\\t', decimal = ',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Feb data with new April 2022 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_dedrifted_dataset_new(dedrifting_method: str = 'exp', envelope_choice: str = 'multienv', window_length: int = 350, alpha: float = 0.022):\n",
    "    '''dedrifting method - choose one of `SavGol`, `exp`, `None`\n",
    "    envelope_choice - choose one of `topenv`, `multienv`\n",
    "    window_length - be careful not to exceed the sample size'''\n",
    "    DF = pd.DataFrame()\n",
    "    for gas, actual_gas, mc in zip(['NO2 ', 'H2S', 'Acet'], ['NO2', 'H2S', 'Acet'],[4,8,8]):\n",
    "        df = dedrift_n_cut(load_data(gas), method=dedrifting_method, how=envelope_choice, window_length=window_length, alpha=alpha, gas=actual_gas)\n",
    "        DF = pd.concat([DF, df.loc[df[\"meas_cycle\"]==mc]], ignore_index=True)\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bparams = {  # classification with catboost\n",
    "    'dedrifting_method': 'exp',\n",
    "    'envelope_choice': \"topenv\",\n",
    "    'window_length': 285,  # 300, #  \n",
    "    'alpha': 0.02171655699670033,\n",
    "    'n_components': 153,\n",
    "    'iterations': 1300,\n",
    "    'depth': 6,\n",
    "    'learning_rate': 0.009372428573829332,\n",
    "    'l2_leaf_reg': 13.155579412870834}\n",
    "\n",
    "start = 7\n",
    "\n",
    "DFn = add_class_column(load_full_dedrifted_dataset_new(bparams['dedrifting_method'], bparams[\"envelope_choice\"], bparams['window_length'], bparams['alpha']))\n",
    "DF = add_class_column(load_full_dedrifted_dataset(bparams['dedrifting_method'], bparams[\"envelope_choice\"], bparams['window_length'], bparams['alpha']))\n",
    "DFn = DFn.assign(meas_cycle = np.zeros(shape=DFn.shape[0]))\n",
    "DFn.NO2.plot().update_layout(height = 150, margin = dict(t=0,l=0,r=0,b=0)).show()\n",
    "\n",
    "do_PCA = True\n",
    "\n",
    "train_y_ = DF.loc[DF['meas_cycle']<=start].iloc[:,402:405].values\n",
    "test_y_ = DFn.iloc[:,402:405].values\n",
    "\n",
    "train_gas = DF.loc[DF['meas_cycle']<=start].loc[:, 'gas'].values\n",
    "test_gas = DFn.loc[:, 'gas'].values  \n",
    "\n",
    "train_y_class = DF.loc[DF['meas_cycle']<=start].loc[:,\"class_\"].values\n",
    "test_y_class = DFn.loc[:,\"class_\"].values\n",
    "\n",
    "# PCA\n",
    "train = DF.loc[DF['meas_cycle']<=start].iloc[:,:402].values\n",
    "test = DFn.iloc[:,:402].values  \n",
    "train_X_class, test_X_class = scale_n_PCA(train, test, n_components=bparams['n_components'], scale=True, scaler=MinMaxScaler(), do_PCA=do_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    iterations= bparams[\"iterations\"],\n",
    "    depth=bparams[\"depth\"],\n",
    "    learning_rate=bparams[\"learning_rate\"],\n",
    "    l2_leaf_reg=bparams[\"l2_leaf_reg\"],\n",
    "    loss_function=\"MultiClassOneVsAll\",\n",
    "    eval_metric=\"TotalF1\",\n",
    ")\n",
    "model.fit(\n",
    "    train_X_class, train_y_class, \n",
    "    eval_set = (test_X_class, test_y_class), \n",
    "    use_best_model=True,\n",
    "    silent=True,\n",
    "    plot=False)\n",
    "plt_CM(model, test_X_class, test_y_class, None, \"2 months validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traditional class\n",
    "train_X, test_X, train_y, test_y = train_test_TS_class(DF, n_components = bparams[\"n_components\"], start = 7)\n",
    "model = CatBoostClassifier(\n",
    "    iterations= bparams[\"iterations\"],\n",
    "    depth=bparams[\"depth\"],\n",
    "    learning_rate=bparams[\"learning_rate\"],\n",
    "    l2_leaf_reg=bparams[\"l2_leaf_reg\"],\n",
    "    loss_function=\"MultiClassOneVsAll\",\n",
    "    eval_metric=\"TotalF1\",\n",
    ")\n",
    "model.fit(\n",
    "    train_X[0], train_y[0], \n",
    "    eval_set = (test_X[0], test_y[0]), \n",
    "    use_best_model=True,\n",
    "    silent=True,\n",
    "    plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_CM(model, test_X[0], test_y[0], None, r\"$\\text{Exponential  } \\alpha = 0.022$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "90/96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bparams = {\n",
    " 'dedrifting_method': 'SavGol',\n",
    " \"envelope_choice\": \"multienv\",\n",
    " 'window_length': 250,\n",
    " \"alpha\": 0.022,\n",
    " 'n_components': 19,\n",
    " 'iterations': 1700,\n",
    " 'depth': 8,\n",
    " 'learning_rate': 0.014,\n",
    " 'l2_leaf_reg': 3}\n",
    "\n",
    "DFn = add_class_column(load_full_dedrifted_dataset_new(bparams['dedrifting_method'], bparams[\"envelope_choice\"], bparams['window_length'], bparams['alpha']))\n",
    "DF = add_class_column(load_full_dedrifted_dataset(bparams['dedrifting_method'], bparams[\"envelope_choice\"], bparams['window_length'], bparams['alpha']))\n",
    "DFn = DFn.assign(meas_cycle = np.zeros(shape=DFn.shape[0]))\n",
    "DFn.NO2.plot().update_layout(height = 150, margin = dict(t=0,l=0,r=0,b=0)).show()\n",
    "\n",
    "do_PCA = True\n",
    "\n",
    "train_y_ = DF.loc[DF['meas_cycle']<=7].iloc[:,402:405].values\n",
    "test_y_ = DFn.iloc[:,402:405].values\n",
    "\n",
    "# PCA\n",
    "train = DF.loc[DF['meas_cycle']<=7].iloc[:,:402].values\n",
    "test = DFn.iloc[:,:402].values  \n",
    "train_X_class, test_X_class = scale_n_PCA(train, test, n_components=bparams['n_components'], scale=True, scaler=MinMaxScaler(), do_PCA=do_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_model = CatBoostRegressor(\n",
    "    iterations= bparams[\"iterations\"],\n",
    "    depth=bparams[\"depth\"],\n",
    "    learning_rate=bparams[\"learning_rate\"],\n",
    "    l2_leaf_reg=bparams[\"l2_leaf_reg\"],\n",
    "    loss_function='MultiRMSE', verbose=0)\n",
    "catboost_model.fit(\n",
    "    train_X_class, train_y_, \n",
    "    eval_set=[(test_X_class, test_y_)], \n",
    "    early_stopping_rounds=200, # need to implement good pruning by starting from the most information train|test pair and not calculate the whole sequence if this pair is pruned\n",
    "    verbose=0)\n",
    "    \n",
    "y_pred = catboost_model.predict(test_X_class)  # add native penalization (sample_weights) according to TLV_TWA values for gases, NO2 - 0.2, H2S - 1, Acet - \n",
    "mse_ = mean_squared_error(test_y_, y_pred, multioutput='raw_values', squared = False) # substitute this with native CatBoost stuff\n",
    "result_df = pd.DataFrame(data = mse_, index = DF.columns[402:405], columns = [\"RMSE\"]).T\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traditional\n",
    "train_y_ = DF.loc[DF['meas_cycle']<=7].iloc[:,402:405].values\n",
    "test_y_ = DF.loc[DF['meas_cycle']>7].iloc[:,402:405].values\n",
    "\n",
    "# PCA\n",
    "train = DF.loc[DF['meas_cycle']<=7].iloc[:,:402].values\n",
    "test = DF.loc[DF['meas_cycle']>7].iloc[:,:402].values  \n",
    "train_X_class, test_X_class = scale_n_PCA(train, test, n_components=bparams['n_components'], scale=True, scaler=MinMaxScaler(), do_PCA=do_PCA)\n",
    "\n",
    "catboost_model = CatBoostRegressor(\n",
    "    iterations= bparams[\"iterations\"],\n",
    "    depth=bparams[\"depth\"],\n",
    "    learning_rate=bparams[\"learning_rate\"],\n",
    "    l2_leaf_reg=bparams[\"l2_leaf_reg\"],\n",
    "    loss_function='MultiRMSE', verbose=0)\n",
    "catboost_model.fit(\n",
    "    train_X_class, train_y_, \n",
    "    eval_set=[(test_X_class, test_y_)], \n",
    "    early_stopping_rounds=200, # need to implement good pruning by starting from the most information train|test pair and not calculate the whole sequence if this pair is pruned\n",
    "    verbose=0)\n",
    "    \n",
    "y_pred = catboost_model.predict(test_X_class)  # add native penalization (sample_weights) according to TLV_TWA values for gases, NO2 - 0.2, H2S - 1, Acet - \n",
    "mse_ = mean_squared_error(test_y_, y_pred, multioutput='raw_values', squared = False) # substitute this with native CatBoost stuff\n",
    "result_df = pd.DataFrame(data = mse_, index = DF.columns[402:405], columns = [\"RMSE\"]).T\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bparams = {  # LSTM\n",
    "    'dedrifting_method': 'SavGol',\n",
    "    'envelope_choice': 'multienv',\n",
    "    'window_length': 300,\n",
    "    'look_back': 29,\n",
    "    'n_components': 112,\n",
    "    'n_units': 32,\n",
    "    'dropout': 0.15,\n",
    "    'learning_rate': 0.010415946044372366,\n",
    "    'batch_size': 128,\n",
    "    'alpha': 0.1  # for compatibility\n",
    "    }\n",
    "DFn = add_class_column(load_full_dedrifted_dataset_new(bparams['dedrifting_method'], bparams[\"envelope_choice\"], bparams['window_length'], bparams['alpha']))\n",
    "DF = add_class_column(load_full_dedrifted_dataset(bparams['dedrifting_method'], bparams[\"envelope_choice\"], bparams['window_length'], bparams['alpha']))\n",
    "DFn = DFn.assign(meas_cycle = np.zeros(shape=DFn.shape[0]))\n",
    "DFn.NO2.plot().update_layout(height = 150, margin = dict(t=0,l=0,r=0,b=0)).show()\n",
    "\n",
    "do_PCA = True\n",
    "\n",
    "train_y_ = DF.loc[DF['meas_cycle']<=8].iloc[:,402:405].values\n",
    "test_y_ = DFn.iloc[:,402:405].values\n",
    "\n",
    "train_gas = DF.loc[DF['meas_cycle']<=8].loc[:, 'gas'].values\n",
    "test_gas = DFn.loc[:, 'gas'].values  \n",
    "\n",
    "train_y_class = DF.loc[DF['meas_cycle']<=8].loc[:,\"class_\"].values\n",
    "test_y_class = DFn.loc[:,\"class_\"].values\n",
    "\n",
    "# PCA\n",
    "train = DF.loc[DF['meas_cycle']<=8].iloc[:,:402].values\n",
    "test = DFn.iloc[:,:402].values  \n",
    "train_X_class, test_X_class = scale_n_PCA(train, test, n_components=bparams['n_components'], scale=True, scaler=MinMaxScaler(), do_PCA=do_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# careful splitting\n",
    "to_weight = False\n",
    "train_X_lstm, train_y_lstm, train_sw = create_RNN_sequences_for_multiple_gases(X = train_X_class, y = train_y_, gas_type = train_gas, look_back = bparams['look_back'], to_weight=to_weight)\n",
    "test_X_lstm, test_y_lstm, test_sw = create_RNN_sequences_for_multiple_gases(X = test_X_class, y = test_y_, gas_type = test_gas, look_back = bparams['look_back'], to_weight=to_weight)\n",
    "\n",
    "model = make_LSTM(\n",
    "    [train_X_lstm], \n",
    "    [train_y_lstm], \n",
    "    optimizer = Adam, \n",
    "    loss = 'mean_squared_error',\n",
    "    n_LSTM_layers = 1,\n",
    "    n_units = bparams['n_units'],\n",
    "    dropout = bparams['dropout'],\n",
    "    learning_rate = bparams['learning_rate'],)\n",
    "# train the model\n",
    "mse_, history_ = fit_LSTM(\n",
    "    model, train_X_lstm, train_y_lstm, test_X_lstm, test_y_lstm, train_sw, test_sw,\n",
    "    epochs = 150,\n",
    "    batch_size= bparams['batch_size'], \n",
    "    return_history=True)\n",
    "plot_history(history_, bparams)\n",
    "y_pred = model.predict(test_X_lstm, batch_size = bparams['batch_size'], \n",
    "                           #workers = 16, use_multiprocessing = True,\n",
    "                           )\n",
    "test_rmse = mean_squared_error(test_y_lstm, y_pred, sample_weight= test_sw, squared = False, multioutput = 'raw_values')\n",
    "result_df = pd.DataFrame(data = test_rmse, index = DF.columns[402:405], columns = [\"RMSE\"]).T\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traditional\n",
    "train_X, test_X, train_y, test_y, train_sw, test_sw = train_test_RNN(DF, \n",
    "                                                    look_back= bparams['look_back'],\n",
    "                                                    n_components= bparams['n_components'],\n",
    "                                                    do_PCA= True,\n",
    "                                                    start= 7)\n",
    "model = make_LSTM(\n",
    "    train_X, \n",
    "    train_y, \n",
    "    optimizer = Adam, \n",
    "    loss = 'mean_squared_error',\n",
    "    n_LSTM_layers = 1,\n",
    "    n_units = bparams['n_units'],\n",
    "    dropout = bparams['dropout'],\n",
    "    learning_rate = bparams['learning_rate'],)\n",
    "# train the model\n",
    "mse = []\n",
    "#for X_train, X_test, y_train, y_test in zip(train_X[::-1], test_X[::-1], train_y[::-1], test_y[::-1]):  # reversed the order so the the trial be pruned earlier and more consistent\n",
    "mse_, history_ = fit_LSTM(\n",
    "    model, train_X[0], train_y[0], test_X[0], test_y[0], train_sw, test_sw,\n",
    "    epochs = 200,\n",
    "    batch_size= bparams['batch_size'], \n",
    "    return_history=True)\n",
    "plot_history(history_, bparams)\n",
    "y_pred = model.predict(test_X[0], batch_size = bparams['batch_size'], \n",
    "                           #workers = 16, use_multiprocessing = True,\n",
    "                           )\n",
    "test_rmse = mean_squared_error(test_y[0], y_pred, sample_weight= test_sw, squared = False, multioutput = 'raw_values')\n",
    "result_df = pd.DataFrame(data = test_rmse, index = DF.columns[402:405], columns = [\"RMSE\"]).T\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### smoothing methods illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"NO2\").I.iloc[201::402]\n",
    "X = (data.index - data.index[0]).total_seconds().values/60\n",
    "y = data.values*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = SavGol(y, window_length = 250) + y.mean()\n",
    "exp = y - forward_backward_exponential_smoothing(y, alpha=0.02) + y.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import match\n",
    "gas = 'NO2'\n",
    "def extract_gas_protocol(df: pd.DataFrame) -> (np.array, np.array):\n",
    "    try:\n",
    "        dfg = df.copy(deep=True)\n",
    "        dfg = dfg.set_index((dfg.index - dfg.index[0]).total_seconds().values/60)\n",
    "        dfg = dfg.assign(new_gas = dfg['MFC_target'].rolling(window=3).apply(lambda x: np.NaN if x[0] == x[1] == x[2] else x[1], raw = True))\n",
    "        dfg = dfg.dropna()\n",
    "        return [dfg.index[0]] + dfg.index.to_list() + [df.index[-1]], [df.MFC_target.iloc[0]] + dfg.new_gas.to_list() + [df.MFC_target.iloc[-1]]\n",
    "    \n",
    "    except AttributeError:\n",
    "        print(\"****************\\nTime and MFC_target are not in df.columns\\n****************\")\n",
    "def color_with_opacity(color: str, opacity: float = 0.2) -> str:\n",
    "    '''Takes a color of any format (hex, rgb) returns rgba with given opacity'''\n",
    "    # Check if the input color is in hex format\n",
    "    if color.startswith(\"#\"):\n",
    "        r, g, b = px.colors.hex_to_rgb(color)\n",
    "    else:\n",
    "        # Check if the input color is in 'rgb(r, g, b)' format\n",
    "        rgb_match = match(r'rgb\\((\\d+), (\\d+), (\\d+)\\)', color)\n",
    "        if rgb_match:\n",
    "            r, g, b = map(int, rgb_match.groups())\n",
    "        else:\n",
    "            raise ValueError(\"Invalid color format. Supported formats: hex or 'rgb(r, g, b)'.\")\n",
    "\n",
    "    rgba_color = f'rgba({r}, {g}, {b}, {opacity})'    \n",
    "    return rgba_color\n",
    "def create_gas_rectangles(gas_index: list, gas_values: list, colorscale, **kwargs) -> list:\n",
    "    '''takes the index and values from extract_gas_protocol function and returns the list of go.layout.Shape dictionaries'''\n",
    "    rectangles = []\n",
    "    unique_gas_values = list(set(gas_values))\n",
    "    colors = dict(zip(unique_gas_values, px.colors.sample_colorscale(colorscale, minmax_scale(unique_gas_values))))\n",
    "    kwargs.setdefault('yref', 'paper')\n",
    "    kwargs.setdefault('y0', 0)\n",
    "    kwargs.setdefault('y1', 1)\n",
    "    for i in range(0, len(gas_index) - 1, 2):\n",
    "        text = '' if gas_values[i]==0 else f'<b>{gas_values[i]:.0f}</b>'  # gas is a global variable  #  ppm {target_gas_dict[gas]}\n",
    "        color = 'rgba(255,255,250,0.3)' if gas_values[i]==0 else color_with_opacity(colors[gas_values[i]], 0.5)  # as well as target_gas_dict\n",
    "        rectangle = dict(type = 'rect', \n",
    "                         xref= 'x', yref= kwargs['yref'], \n",
    "                         x0= gas_index[i], x1= gas_index[i+1], y0= kwargs['y0'], y1=kwargs[\"y1\"], \n",
    "                         fillcolor = color, line_width= 0, \n",
    "                         label = dict(text= text, textangle= -0, xanchor= 'center', textposition= 'bottom center', padding= 20, font_color='black'),\n",
    "                         layer = 'above',\n",
    "                         )\n",
    "        rectangles.append(go.layout.Shape(rectangle))\n",
    "    return rectangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectangles = create_gas_rectangles(*extract_gas_protocol(load_data(\"NO2\")), 'Sunset_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "plotly = px.colors.qualitative.Plotly\n",
    "fig.add_trace(go.Scatter(x = X, y = y, name= 'raw data 25 V envelope', line_color = plotly[1]))\n",
    "fig.add_trace(go.Scatter(x = X, y = exp, name= 'exponential smoothing <br>baseline compensation', line_color = plotly[0]))\n",
    "fig.add_trace(go.Scatter(x = X, y = sg, name= 'Savitzky-Golay smoothing <br>baseline compensation', line_color = plotly[5]))\n",
    "fig.update_xaxes(mirror=True)\n",
    "fig.update_yaxes(mirror=True)\n",
    "fig.update_traces(line_width = 3)\n",
    "fig.update_layout(template = 'simple_white',\n",
    "                  font_family = \"Times New Roman\",\n",
    "                  font_size = 22,\n",
    "                  yaxis_nticks = 3,\n",
    "                  xaxis_nticks = 5,\n",
    "                  yaxis_title = \"<b>Current</b>, mA\",\n",
    "                  xaxis_title = \"<b>Time</b>, min\",\n",
    "                  legend_orientation = 'v',\n",
    "                  legend_xanchor = 'right',\n",
    "                  legend_x = 1,\n",
    "                  legend_yanchor = 'bottom',\n",
    "                  legend_y = 0.14,\n",
    "                  width = 1122,\n",
    "                  margin = dict(t=5, b=5, r=5, l=5),\n",
    "                  shapes =rectangles,\n",
    "                  yaxis_range = [16.2, 18.8],\n",
    "                  xaxis_range = [0, 945])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"../Graphs/Suppl/Smoothing illustration.png\", scale = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(DF.iloc[:,402:405].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_sequences(data: np.ndarray, look_back: int = 50):\n",
    "    num_samples, num_features = data.shape\n",
    "    num_sequences = num_samples - look_back + 1\n",
    "    \n",
    "    sequences = np.zeros((num_sequences, look_back, num_features))\n",
    "    \n",
    "    for i in range(look_back):\n",
    "        sequences[:, i] = data[i:num_samples - look_back + 1 + i]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_RNN(df_: pd.DataFrame, look_back: int = 50, n_components: int = 5, start: int = 1) -> (list, list, list, list):\n",
    "    '''start: int, meas_cycle to start with\n",
    "    returns 4 lists for train/test x/y. Each element = split'''\n",
    "    if not 'meas_cycle' in df_.columns:\n",
    "        print(\"wrong dataframe: no meas_cycle column\")\n",
    "        return None\n",
    "    else:\n",
    "        n_splits = len(df_.meas_cycle.unique())\n",
    "        train_X, test_X, train_y, test_y = [], [], [], []\n",
    "        for split in range(start,n_splits-1):  # starting from 1 cause the first meas cycle was cut\n",
    "            # PCA\n",
    "            train = df_.loc[df_['meas_cycle']<=split].iloc[:,:402].values\n",
    "            test = df_.loc[df_['meas_cycle']==split+1].iloc[:,:402].values\n",
    "            train_pca, test_pca = scale_n_PCA(train, test, n_components=n_components, scale=True, scaler=MinMaxScaler(), do_PCA=True)\n",
    "            train_X.append(train_pca)\n",
    "            train_y.append(df_.loc[df_['meas_cycle']<=split].iloc[:,402:405].values)\n",
    "            test_X.append(test_pca)\n",
    "            test_y.append(df_.loc[df_['meas_cycle']==split+1].iloc[:,402:405].values)\n",
    "        return train_X, test_X, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickplot(df, step: int = 6):\n",
    "    stepping = slice(0, df.shape[0], step)\n",
    "    y = df.loc[:,'I'].iloc[stepping]\n",
    "    x = df.index[stepping]\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scattergl(x=x, y=y))\n",
    "    fig.update_layout(hovermode = False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quickplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gas_protocol(df: pd.DataFrame) -> (np.array, np.array):\n",
    "    try:\n",
    "        dfg = df.assign(new_gas = df['MFC_target'].rolling(window=3).apply(lambda x: np.NaN if x[0] == x[1] == x[2] else x[1], raw = True))\n",
    "        dfg.iloc[0, -1] = df.MFC_target.iloc[0]\n",
    "        dfg.iloc[-1, -1] = df.MFC_target.iloc[-1]\n",
    "        dfg = dfg.dropna()\n",
    "        return dfg.index.to_list(), dfg.new_gas.to_list()\n",
    "    \n",
    "    except AttributeError:\n",
    "        print(\"****************\\nTime and MFC_target are not in df.columns\\n****************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = extract_gas_protocol(load_data('NO2 '))\n",
    "fig = px.line(x=x, y=y)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.meas_cycle.loc[df[\"meas_cycle\"]<5].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
