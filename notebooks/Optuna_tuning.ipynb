{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_slice,\n",
    "    plot_parallel_coordinate,\n",
    "    plot_contour\n",
    ")\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'  # or 'browser'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78408909",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"lstm_regression_v1\"\n",
    "storage = \"sqlite:///optuna_studies.db\"\n",
    "\n",
    "study = optuna.load_study(\n",
    "    study_name=study_name,\n",
    "    storage=storage\n",
    ")\n",
    "\n",
    "print(f\"Study: {study.study_name}\")\n",
    "print(f\"Direction: {study.direction}\")\n",
    "print(f\"Best value: {study.best_value:.6f}\")\n",
    "print(f\"Best params: {study.best_params}\")\n",
    "print(f\"Trials: {len(study.trials)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78165a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_optimization_history(study)\n",
    "fig.update_layout(\n",
    "    title=\"Optimization History - LSTM Regression\",\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b3ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_param_importances(\n",
    "    study,\n",
    "    evaluator=optuna.importance.FanovaImportanceEvaluator()\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Hyperparameter Importance (fANOVA)\",\n",
    "    height=500,\n",
    "    xaxis_title=\"Importance Score\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(study.trials[0], 'duration'):\n",
    "    fig = plot_param_importances(\n",
    "        study,\n",
    "        target=lambda t: t.duration.total_seconds(),\n",
    "        target_name=\"duration\"\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=\"Hyperparameter Impact on Computation Time\",\n",
    "        height=500\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_slice(study)\n",
    "fig.update_layout(\n",
    "    title=\"Hyperparameter Slice Plots\",\n",
    "    height=600\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f9399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_params = ['look_back', 'n_components', 'learning_rate', 'dropout']\n",
    "fig = plot_slice(study, params=key_params)\n",
    "fig.update_layout(\n",
    "    title=\"Key Hyperparameter Effects\",\n",
    "    height=400\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642011fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_parallel_coordinate(\n",
    "    study,\n",
    "    params=['look_back', 'n_components', 'n_layers', 'n_units', 'dropout', 'learning_rate']\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Parallel Coordinates - All Hyperparameters\",\n",
    "    height=600\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eadad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_contour(\n",
    "    study,\n",
    "    params=['look_back', 'n_components']\n",
    ")\n",
    "fig.update_layout(title=\"Look Back vs N Components\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_contour(\n",
    "    study,\n",
    "    params=['learning_rate', 'dropout']\n",
    ")\n",
    "fig.update_layout(title=\"Learning Rate vs Dropout\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58fe699",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BEST TRIAL ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Trial number: {best_trial.number}\")\n",
    "print(f\"Value (RMSE): {best_trial.value:.6f}\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nUser Attributes:\")\n",
    "for key, value in best_trial.user_attrs.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb31f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(f\"Complete trials: {len(complete_trials)}\")\n",
    "print(f\"Pruned trials: {len(pruned_trials)}\")\n",
    "print(f\"Pruning rate: {len(pruned_trials)/len(study.trials)*100:.1f}%\")\n",
    "\n",
    "# Visualize when pruning happened\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "pruned_steps = [len(t.intermediate_values) for t in pruned_trials]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=pruned_steps,\n",
    "    name='Pruned Trials',\n",
    "    nbinsx=20\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=\"Distribution of Pruning Steps\",\n",
    "    xaxis_title=\"Step When Pruned\",\n",
    "    yaxis_title=\"Count\",\n",
    "    height=400\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract fold RMSEs from user attributes\n",
    "fold_data = []\n",
    "for trial in complete_trials:\n",
    "    fold_rmses = [\n",
    "        trial.user_attrs.get(f'fold_{i}_rmse', np.nan)\n",
    "        for i in range(10)  # Adjust based on your max folds\n",
    "    ]\n",
    "    # Remove nans\n",
    "    fold_rmses = [x for x in fold_rmses if not np.isnan(x)]\n",
    "    if fold_rmses:\n",
    "        fold_data.append({\n",
    "            'trial': trial.number,\n",
    "            'mean_rmse': trial.value,\n",
    "            'std_rmse': np.std(fold_rmses),\n",
    "            'cv_stability': np.std(fold_rmses) / np.mean(fold_rmses)\n",
    "        })\n",
    "\n",
    "stability_df = pd.DataFrame(fold_data)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=stability_df['mean_rmse'],\n",
    "    y=stability_df['cv_stability'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color=stability_df['trial'],\n",
    "        colorscale='Viridis',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"Trial #\")\n",
    "    ),\n",
    "    text=[f\"Trial {t}\" for t in stability_df['trial']],\n",
    "    hovertemplate='<b>%{text}</b><br>Mean RMSE: %{x:.4f}<br>CV Stability: %{y:.4f}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Cross-Validation Stability vs Performance\",\n",
    "    xaxis_title=\"Mean RMSE\",\n",
    "    yaxis_title=\"CV Stability (Std/Mean)\",\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014728e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "best_config = {\n",
    "    'best_trial_number': best_trial.number,\n",
    "    'best_value': float(best_trial.value),\n",
    "    'best_params': best_trial.params,\n",
    "    'timestamp': str(best_trial.datetime_complete)\n",
    "}\n",
    "\n",
    "with open('../configs/best_lstm_regression_params.yaml', 'w') as f:\n",
    "    yaml.dump(best_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"âœ“ Saved best parameters to configs/best_lstm_regression_params.yaml\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
